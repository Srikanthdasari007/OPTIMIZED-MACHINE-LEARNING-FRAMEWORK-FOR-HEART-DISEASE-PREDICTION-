{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu95ey_otOHf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report,confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "import shap\n",
    "\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "gQDwC1r8u26d",
    "outputId": "5e9222d5-e151-460a-d348-7eef8577bc83"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMHNvk4IvGeW",
    "outputId": "d0f0957c-15c3-4092-d8f3-b1298dd645fc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'The dataset has {data.shape[0]} rows and {data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huRlpfYzvIM1",
    "outputId": "8b715f98-dd42-4021-9b4d-d5125f6c4aa0"
   },
   "outputs": [],
   "source": [
    "print(f'The dataset has {data.isna().sum().sum()} null values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "rYd61MZbvM-7",
    "outputId": "00b8c6a9-f20b-42d9-a2da-7bb016644b50"
   },
   "outputs": [],
   "source": [
    "def plot_nulls(data, title, x_axis_label, y_axis_label):\n",
    "\n",
    "    # number of nulls for each column\n",
    "    data_nulls = (data.apply(lambda x: x.isnull().value_counts()).T[True]/len(data)*100).reset_index(name='count')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(data_nulls, x=\"index\", y=\"count\", alpha=0.3)\n",
    "\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(x_axis_label, fontsize=15)\n",
    "    plt.xticks(rotation=45, fontsize=12)\n",
    "    plt.ylabel(y_axis_label, fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_nulls(data,\"Null Values in the Dataset\", 'features', '% of null values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4e2oRJqvRFA",
    "outputId": "a3402729-3ae1-4d94-8b62-32fba72162e6"
   },
   "outputs": [],
   "source": [
    "print(f'There are {data.duplicated().sum()} duplicate rows in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTKVwpMOvZOj",
    "outputId": "e6b1a7c3-0eea-479c-963a-effe0b17322d"
   },
   "outputs": [],
   "source": [
    "cat_features = [col for col in data.columns if data[col].dtypes == 'O']\n",
    "\n",
    "print(f'feature           cardinality')\n",
    "for cat in cat_features:\n",
    "    print('{:22s} {:2s}'.format(cat, str(data[cat].nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TFKEnCzavd8N",
    "outputId": "e9ccbae4-41c6-49f4-89e5-5ef97ab0d265"
   },
   "outputs": [],
   "source": [
    "has_stroke = {0:'no stroke', 1:'stroke'}\n",
    "data['has_stroke'] = data['stroke'].map(has_stroke)\n",
    "\n",
    "has_hypertension = {1:'hypertension', 0:'no hypertension'}\n",
    "data['has_hypertension'] = data['hypertension'].map(has_hypertension)\n",
    "\n",
    "has_heart_disease = {1:'heart disease', 0:'no heart disease'}\n",
    "data['has_heart_disease'] = data['heart_disease'].map(has_heart_disease)\n",
    "\n",
    "\n",
    "### Figures ###\n",
    "bigfig = plt.figure(figsize=(12,6))\n",
    "\n",
    "(top, central, bottom) = bigfig.subfigures(3,1)\n",
    "\n",
    "### Top figure ###\n",
    "top.subplots_adjust(left=.1, right=.9, wspace=.4, hspace=.4)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=data, x='has_stroke', alpha=0.2, ax=ax1)\n",
    "ax1.set_title('Stroke (Target Variable)', size=20)\n",
    "\n",
    "ax2 = sns.histplot(data=data, x='gender', hue='stroke', alpha=0.2, ax=ax2)\n",
    "ax2.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax2.set_title('Gender', size=20)\n",
    "\n",
    "ax3 = sns.histplot(data=data, x='ever_married', hue='stroke', alpha=0.2, ax=ax3)\n",
    "ax3.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax3.set_title('Has Ever Been Married?', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "### Central figure ###\n",
    "central.subplots_adjust(left=.1, right=.9, wspace=.4, hspace=.4)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=data, x='Residence_type', hue='stroke', alpha=0.2, ax=ax1)\n",
    "ax1.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax1.set_title('Residence type', size=20)\n",
    "\n",
    "ax2 = sns.histplot(data=data, x='has_hypertension', hue='stroke', alpha=0.2, ax=ax2)\n",
    "ax2.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax2.set_title('Has Hypertension?', size=20)\n",
    "\n",
    "ax3 = sns.histplot(data=data, x='has_heart_disease', hue='stroke', alpha=0.2, ax=ax3)\n",
    "ax3.legend(title='stroke', loc='upper left', labels=['yes', 'no'])\n",
    "ax3.set_title('Has Heart Disease?', size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "### Bottom figure ###\n",
    "bottom.subplots_adjust(left=.1, right=.9, wspace=.4, hspace=.4)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=data, x='work_type', hue='stroke', alpha=0.2, ax=ax1)\n",
    "ax1.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, size=10)\n",
    "ax1.set_title('Job Type', size=20)\n",
    "\n",
    "ax2 = sns.histplot(data=data, x='smoking_status', hue='stroke', alpha=0.2, ax=ax2)\n",
    "ax2.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, size=10)\n",
    "ax2.set_title('Smoking Status', size=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B9kM-rr4vocd",
    "outputId": "8082b4e9-db4d-4637-f25a-aa4de4e3e2f0"
   },
   "outputs": [],
   "source": [
    "### Figures ###\n",
    "bigfig = plt.figure(figsize=(12,6))\n",
    "\n",
    "(top, central, bottom) = bigfig.subfigures(3,1)\n",
    "\n",
    "### Top figure ###\n",
    "top.subplots_adjust(left=.1, right=.9, wspace=.4, hspace=.4)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=data, x='avg_glucose_level', hue='stroke', bins=20, alpha=0.2, ax=ax1)\n",
    "ax1.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax1.set_title('Histogram of Avg Glucose Level', size=18)\n",
    "\n",
    "ax2 = sns.kdeplot(data=data.loc[data.stroke == 0], x='avg_glucose_level', label='no stroke', ax=ax2)\n",
    "ax2 = sns.kdeplot(data=data.loc[data.stroke == 1], x='avg_glucose_level', label='stroke', ax=ax2)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_title('Density Distribution of Avg Glucose Level', size=18)\n",
    "\n",
    "plt.suptitle('Average Glucose Level: Histogram and Density Distribution', size=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "### Central figure ###\n",
    "central.subplots_adjust(left=.1, right=.9, wspace=.4, hspace=.4)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=data, x='age', hue='stroke', bins=20, alpha=0.2, ax=ax1)\n",
    "ax1.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax1.set_title('Histogram of Age', size=18)\n",
    "\n",
    "ax2 = sns.kdeplot(data=data.loc[data.stroke == 0], x='age', label='no stroke', ax=ax2)\n",
    "ax2 = sns.kdeplot(data=data.loc[data.stroke == 1], x='age', label='stroke', ax=ax2)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_title('Density Distribution of Age', size=18)\n",
    "\n",
    "plt.suptitle('Age: Histogram and Density Distribution', size=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "### Bottom figure ###\n",
    "bottom.subplots_adjust(left=.1, right=.9, wspace=.4, hspace=.4)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=data, x='bmi', hue='stroke', bins=20, alpha=0.2, ax=ax1)\n",
    "ax1.legend(title='stroke', loc='upper right', labels=['yes', 'no'])\n",
    "ax1.set_title('Histogram of Body Mass Index', size=18)\n",
    "\n",
    "ax2 = sns.kdeplot(data=data.loc[data.stroke == 0], x='bmi', label='no stroke', ax=ax2)\n",
    "ax2 = sns.kdeplot(data=data.loc[data.stroke == 1], x='bmi', label='stroke', ax=ax2)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_title('Density Distribution of Body Mass Index', size=18)\n",
    "\n",
    "plt.suptitle('Body Mass Index (BMI): Histogram and Density Distribution', size=25)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPdG8odwvxOx"
   },
   "outputs": [],
   "source": [
    "\n",
    "data.drop(['has_stroke', 'has_hypertension', 'has_heart_disease'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fiGYCnkSwFNz",
    "outputId": "f48e5a5e-5e98-46fe-de6a-66201195dc61"
   },
   "outputs": [],
   "source": [
    "data_no_nulls = data.dropna(axis=0)\n",
    "\n",
    "num_columns = [col for col in data.columns if (data[col].dtypes != 'O') & (col not in ['id', 'bmi'])]\n",
    "\n",
    "print(f'feature             mean (w/ nulls)             mean (no nulls)             rel. difference (%)\\n')\n",
    "def compare_mean(data, data_nn):\n",
    "    for col in num_columns:\n",
    "        print('{:20s} {:8.3f} {:28.3f} {:30.1f}'.format(col, data[col].mean(), data_nn[col].mean(), abs(data[col].mean() - data_nn[col].mean()) / data[col].mean() * 100))\n",
    "\n",
    "compare_mean(data, data_no_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kH4ngccwH1V",
    "outputId": "2e732c44-accc-4b37-fbb5-a5a5b295aaa9"
   },
   "outputs": [],
   "source": [
    "print(f\"Stroke events: {data['stroke'].value_counts()[1]} (w/ nulls), {data_no_nulls['stroke'].value_counts()[1]} (no nulls).\")\n",
    "\n",
    "print(f\"Relative difference in the number of strokes = {(data['stroke'].value_counts()[1] - data_no_nulls['stroke'].value_counts()[1]) / data['stroke'].value_counts()[1] * 100:.0f}%\")\n",
    "\n",
    "print(f\"Relative difference in the dataset length = {(len(data) - len(data_no_nulls)) / len(data) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "CBi5aAwswKKV",
    "outputId": "6a2f43ec-f063-4c95-d298-e73e676e9add"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_features = ['age', 'bmi', 'avg_glucose_level']\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3, figsize=(12,5))\n",
    "\n",
    "ax1 = sns.boxplot(data[num_features[0]],ax=ax1)\n",
    "ax1.set_title('Boxplot of '+str(num_features[0]),fontsize=16)\n",
    "\n",
    "ax2 = sns.boxplot(data[num_features[1]],ax=ax2)\n",
    "ax2.set_title('Boxplot of '+str(num_features[1]),fontsize=16)\n",
    "\n",
    "ax3 = sns.boxplot(data[num_features[2]],ax=ax3)\n",
    "ax3.set_title('Boxplot of '+str(num_features[2]),fontsize=16)\n",
    "\n",
    "plt.suptitle('Boxplots of the Numerical Variables',size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eu-q_t1gwN-O",
    "outputId": "eb290bf3-6ef5-4a5a-e899-5d1f1022b0c6"
   },
   "outputs": [],
   "source": [
    "outliers_perc = []\n",
    "\n",
    "def outliers_perc_search(data, num_features):\n",
    "    for k,v in data[num_features].items():\n",
    "        # Column must be of numeric type (not object)\n",
    "        if data[k].dtype != 'O':\n",
    "            q1 = v.quantile(0.25)\n",
    "            q3 = v.quantile(0.75)\n",
    "            irq = q3 - q1\n",
    "            v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
    "            perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n",
    "            out_tuple = (k,int(perc))\n",
    "            outliers_perc.append(out_tuple)\n",
    "            print(\"Column %s outliers = %.1f%%\" % (k,perc))\n",
    "\n",
    "outliers_perc_search(data, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFiNy5eJwSUd"
   },
   "outputs": [],
   "source": [
    "capped_cols = ['bmi', 'avg_glucose_level']\n",
    "\n",
    "def remove_outliers_iqr(data, column):\n",
    "\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter the data\n",
    "    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "for col in capped_cols:\n",
    "    data = remove_outliers_iqr(data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "2-zWC7o7wSuP",
    "outputId": "7eabb695-3a52-4443-a426-958cb7afa540"
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(ncols=2, figsize=(12,5))\n",
    "\n",
    "ax1 = sns.boxplot(y=data[capped_cols[0]],ax=ax1)\n",
    "ax1.set_title('Boxplot of '+str(capped_cols[0]),fontsize=16)\n",
    "\n",
    "ax2 = sns.boxplot(y=data[capped_cols[1]],ax=ax2)\n",
    "ax2.set_title('Boxplot of '+str(capped_cols[1]),fontsize=16)\n",
    "\n",
    "plt.suptitle('Boxplots of the Numerical Variables',size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ok0S5i5hwUUW"
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=42)\n",
    "\n",
    "X = data.drop(['stroke', 'id'], axis=1)\n",
    "y = data['stroke']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReVzvgOqwYTN"
   },
   "outputs": [],
   "source": [
    "train_stroke_index = y_train[y_train == 0].index.to_list()\n",
    "train_no_stroke_index = y_train[y_train == 1].index.to_list()\n",
    "\n",
    "test_stroke_index = y_test[y_test == 0].index.to_list()\n",
    "test_no_stroke_index = y_test[y_test == 1].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuHCel4zwcO3"
   },
   "outputs": [],
   "source": [
    "bmi_stroke_mean = X_train.loc[train_stroke_index]['bmi'].mean()\n",
    "bmi_no_stroke_mean = X_train.loc[train_no_stroke_index]['bmi'].mean()\n",
    "\n",
    "X_train.loc[(X_train['bmi'].isna()) & (X_train.index.isin(train_stroke_index)), 'bmi'] = bmi_stroke_mean\n",
    "X_train.loc[(X_train['bmi'].isna()) & (X_train.index.isin(train_no_stroke_index)), 'bmi'] = bmi_no_stroke_mean\n",
    "\n",
    "X_test.loc[(X_test['bmi'].isna()) & (X_test.index.isin(test_stroke_index)), 'bmi'] = bmi_stroke_mean\n",
    "X_test.loc[(X_test['bmi'].isna()) & (X_test.index.isin(test_no_stroke_index)), 'bmi'] = bmi_no_stroke_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoJ34Nh_0r8M",
    "outputId": "ab81e55f-3f33-4108-eec0-7026cfc9ba68"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check for null values in the entire DataFrame\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtdAEWbXwftX",
    "outputId": "63dc083a-8053-4b39-c39c-cef2cf8589bb"
   },
   "outputs": [],
   "source": [
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if\n",
    "                   set(X_test[col]).issubset(set(X_train[col]))]\n",
    "\n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "\n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrYueatjwiJB"
   },
   "outputs": [],
   "source": [
    "LE_cols = ['ever_married', 'Residence_type',  'gender']\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "\n",
    "for col in LE_cols:\n",
    "    X_train[col] = encoder1.fit_transform(X_train[col])\n",
    "    X_test[col]  = encoder1.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16lPX9kNwtf9"
   },
   "outputs": [],
   "source": [
    "### One-hot encoding ###\n",
    "OHE_cols = ['work_type', 'smoking_status']\n",
    "\n",
    "#encoder2 = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder2 = OneHotEncoder(sparse_output=False, handle_unknown='ignore').set_output(transform=\"pandas\")\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "OHE_train = pd.DataFrame(encoder2.fit_transform(X_train[OHE_cols]))\n",
    "OHE_test  = pd.DataFrame(encoder2.transform(X_test[OHE_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OHE_train.index = X_train.index\n",
    "OHE_test.index  = X_test.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_train = X_train.drop(OHE_cols, axis=1)\n",
    "num_test  = X_test.drop(OHE_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OHE_X_train = pd.concat([num_train, OHE_train], axis=1)\n",
    "OHE_X_test  = pd.concat([num_test, OHE_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "EZmnC5KNwxSJ",
    "outputId": "9b52e618-4a54-4614-b603-9f2509b4ecd8"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "OHE_X_train[['age', 'bmi', 'avg_glucose_level']] = scaler.fit_transform(OHE_X_train[['age', 'bmi', 'avg_glucose_level']])\n",
    "OHE_X_test[['age', 'bmi', 'avg_glucose_level']]  = scaler.transform(OHE_X_test[['age', 'bmi', 'avg_glucose_level']])\n",
    "\n",
    "OHE_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "id": "QXnfTWL_9Pmz",
    "outputId": "0bdd64ff-bd88-47b1-9425-0bc585fab2dc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = OHE_X_train.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lN8MZVYcw0PP",
    "outputId": "ea34ba7a-844e-4baa-8481-86fb4a4b8ecd"
   },
   "outputs": [],
   "source": [
    "# OverSampler object\n",
    "ros = SMOTE(random_state=42, sampling_strategy='minority')\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(OHE_X_train, y_train)\n",
    "\n",
    "X_resampled.shape, y_resampled.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxqqOlNNzz5p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "fn1w-0wTw2-w",
    "outputId": "f38e776a-9395-4c2d-fd33-7a32b955e9d2"
   },
   "outputs": [],
   "source": [
    "y_train_has_stroke     = y_train.map(has_stroke)\n",
    "y_resampled_has_stroke = y_resampled.map(has_stroke)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "ax1 = sns.histplot(data=y_train_has_stroke, alpha=0.2, ax=ax1)\n",
    "ax1.set_title('Stroke (Target Variable, No Resampling)', size=18)\n",
    "\n",
    "ax2 = sns.histplot(data=y_resampled_has_stroke, alpha=0.2, ax=ax2)\n",
    "ax2.set_title('Stroke (Target Variable with Resampling)', size=18)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760,
     "referenced_widgets": [
      "0c79b4da37ef401289395b9008b867af",
      "86923eb181bf4dc28035e6597ed289e7",
      "a6417abb50ee413a85d442b31b26b092",
      "b3e7832a43bd476cb3f8cc02cba0b744",
      "573a1283bd674120934f530af00f7956",
      "8d23759d78cf41a29198cb06270d8a99",
      "c4ba0789d0ae4d75bf2471f8b6212e59",
      "1f6de437a3be487e9fea74640cb0b87c",
      "c582a26924a940f7ae3ac760c2b9dc08",
      "908070fb509e4f6cb2851f5371204363",
      "d7d8283eac3b4668977f488cbec78c35"
     ]
    },
    "id": "rb-7v-tXy0IT",
    "outputId": "b0741159-faf8-4744-e9d1-ee0e664227dc"
   },
   "outputs": [],
   "source": [
    "def predict_fn(X):\n",
    "\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "# Background data for the explainer (a sample from your data)\n",
    "background_data = shap.sample(X_resampled, 1000)  # Using 1000 samples for the background\n",
    "\n",
    "# Create the KernelExplainer\n",
    "explainer = shap.KernelExplainer(predict_fn, background_data)\n",
    "\n",
    "# Calculate SHAP values for a subset of your data (e.g., the first 1000 samples)\n",
    "shap_values = explainer.shap_values(X_resampled[:1000])\n",
    "\n",
    "# Get feature importances based on mean absolute SHAP values\n",
    "feature_importances = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "# Print or visualize feature importances (example using a bar plot)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(X_resampled.columns, feature_importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Feature Importance (Mean Absolute SHAP Value)\")\n",
    "plt.title(\"SHAP Feature Importance without Model Training\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "km3XiEq_i_PN",
    "outputId": "acc2479e-c441-4b7d-fb87-5f3388c4502a"
   },
   "outputs": [],
   "source": [
    "# Select top features based on importance (example: selecting top 5 features)\n",
    "top_features = X_resampled.columns[np.argsort(feature_importances)[-5:]]\n",
    "print(\"Top 5 features:\", top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8pYptEz2Oxm"
   },
   "outputs": [],
   "source": [
    "# Use only the top 5 selected features from SHAP\n",
    "top_features = ['Residence_type', 'smoking_status_never smoked', 'bmi', 'avg_glucose_level', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLFjtKoB2VmQ"
   },
   "outputs": [],
   "source": [
    "# Select only top 5 features for training & testing\n",
    "X_train_selected = X_resampled[top_features]\n",
    "X_test_selected = OHE_X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfhwf6v9S2dW",
    "outputId": "0031292d-71dc-4354-d622-009db7e416c1"
   },
   "outputs": [],
   "source": [
    "# Updated models with fine-tuned parameters\n",
    "log_reg = LogisticRegression(C=1, solver='liblinear', random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_estimators=150,\n",
    "                    max_depth=4, learning_rate=0.1, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "svm = SVC(kernel='rbf', C=2.0, gamma=0.01, probability=True, random_state=42)\n",
    "\n",
    "# Model dictionary\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Random Forest\": rf,\n",
    "    \"XGBoost\": xgb,\n",
    "    \"K-Nearest Neighbors\": knn,\n",
    "    \"Support Vector Machine\": svm\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} on top 5 features...\")\n",
    "\n",
    "    model.fit(X_train_selected, y_resampled)  # Train the model\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_selected)\n",
    "    y_test_pred = model.predict(X_test_selected)\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\n{name} - Training Set Performance:\\n\")\n",
    "    print(classification_report(y_resampled, y_train_pred))\n",
    "\n",
    "    print(f\"\\n{name} - Test Set Performance:\\n\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsSqPomLUXth",
    "outputId": "8f2252ed-fb17-4522-ee63-3b8301084e13"
   },
   "outputs": [],
   "source": [
    "# Whale Optimization Algorithm (WOA) for Hyperparameter Tuning\n",
    "def whale_optimization(obj_function, bounds, whales=10, iterations=10, a=2):\n",
    "    dim = len(bounds)\n",
    "    positions = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds], (whales, dim))\n",
    "    fitness = np.array([obj_function(ind) for ind in positions])\n",
    "    best_idx = np.argmax(fitness)\n",
    "    best_position = positions[best_idx]\n",
    "\n",
    "    for t in range(iterations):\n",
    "        a_t = a - t * (a / iterations)\n",
    "        for i in range(whales):\n",
    "            A = 2 * a_t * np.random.rand(dim) - a_t\n",
    "            C = 2 * np.random.rand(dim)\n",
    "            p = np.random.rand()\n",
    "            if p < 0.5:\n",
    "                D = np.abs(C * best_position - positions[i])\n",
    "                positions[i] = best_position - A * D\n",
    "            else:\n",
    "                positions[i] = (best_position + positions[i]) / 2\n",
    "\n",
    "            # Ensure values stay within bounds\n",
    "            positions[i] = np.clip(positions[i], [b[0] for b in bounds], [b[1] for b in bounds])\n",
    "\n",
    "        fitness = np.array([obj_function(ind) for ind in positions])\n",
    "        best_idx = np.argmax(fitness)\n",
    "        best_position = positions[best_idx]\n",
    "\n",
    "    return best_position, fitness[best_idx]\n",
    "\n",
    "# Objective function for XGBoost\n",
    "def objective_function(params):\n",
    "    n_estimators, max_depth, learning_rate, subsample = params\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth) if max_depth > 0 else None\n",
    "    learning_rate = max(0.01, min(0.3, learning_rate))  # Keep learning rate between 0.01 and 0.3\n",
    "    subsample = max(0.3, min(1.0, subsample))  # Ensure subsample is between 0.3 and 1.0\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                        learning_rate=learning_rate, subsample=subsample,\n",
    "                        use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    clf.fit(X_resampled, y_resampled)\n",
    "    predictions = clf.predict(OHE_X_test)\n",
    "    return accuracy_score(y_test, predictions)\n",
    "\n",
    "# Define hyperparameter bounds for WOA (based on given XGBoost params)\n",
    "bounds = [(100, 200), (3, 6), (0.05, 0.2), (0.5, 1.0)]  # Tuned to match base parameters\n",
    "\n",
    "# Run Whale Optimization Algorithm\n",
    "best_params, best_fitness = whale_optimization(objective_function, bounds)\n",
    "\n",
    "best_n_estimators, best_max_depth, best_learning_rate, best_subsample = best_params\n",
    "best_n_estimators = int(best_n_estimators)\n",
    "best_max_depth = int(best_max_depth) if best_max_depth > 0 else None\n",
    "\n",
    "# Train the optimized XGBoost model\n",
    "best_xgb = XGBClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth,\n",
    "                         learning_rate=best_learning_rate, subsample=best_subsample,\n",
    "                         use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "best_xgb.fit(X_resampled, y_resampled)\n",
    "final_predictions = best_xgb.predict(OHE_X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_predictions) * 100\n",
    "\n",
    "# Output results\n",
    "print(\"Optimized XGBoost Classifier with Whale Optimization Algorithm\")\n",
    "print(f\"Best Parameters: n_estimators={best_n_estimators}, max_depth={best_max_depth}, \")\n",
    "print(f\"      learning_rate={best_learning_rate:.3f}, subsample={best_subsample:.2f}\")\n",
    "print(f\"Final Accuracy: {final_accuracy:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Y0n2HLNfih3B",
    "outputId": "cef13285-cfb7-4433-882f-810caa88c848"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, final_predictions)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "cB65pbhWt6BA",
    "outputId": "eb5fc20d-304b-4944-b284-f4453939d2d5"
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3, figsize=(12,6))\n",
    "\n",
    "ax1 = PartialDependenceDisplay.from_estimator(best_xgb, OHE_X_test, ['age'], ax=ax1)\n",
    "plt.title('Partial Dependence of Age', size=16)\n",
    "\n",
    "ax2 = PartialDependenceDisplay.from_estimator(best_xgb, OHE_X_test, ['bmi'], ax=ax2)\n",
    "plt.title('Partial Dependence of BMI', size=16)\n",
    "\n",
    "ax3 = PartialDependenceDisplay.from_estimator(best_xgb, OHE_X_test, ['avg_glucose_level'], ax=ax3)\n",
    "plt.title('Partial Dependence of Glucose Level', size=14)\n",
    "\n",
    "plt.suptitle('Partial Dependence Plots from XGB', size=30)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DvxWmc6u4Lg-",
    "outputId": "07505619-267e-4762-c77f-25213c79a0ca"
   },
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer with trained model\n",
    "explainer = shap.Explainer(best_xgb, X_resampled)\n",
    "\n",
    "# Compute SHAP values for test data\n",
    "shap_values = explainer(OHE_X_test)\n",
    "\n",
    "# Summary plot - Feature Importance & Interactions\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, OHE_X_test, show=True)\n",
    "\n",
    "# Interaction Summary Plot (for deeper feature interactions)\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, OHE_X_test, plot_type=\"bar\")\n",
    "\n",
    "# Individual feature interaction (Example: 'bmi' vs. others)\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.dependence_plot(\"bmi\", shap_values.values, OHE_X_test, interaction_index=\"avg_glucose_level\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qce16radclZg"
   },
   "source": [
    "CLINICAL FACTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3XRTQ-mceXb"
   },
   "outputs": [],
   "source": [
    "heart_data = pd.read_csv(\"Cleaveland.CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uelxzN66c82b",
    "outputId": "dd9d38cb-5a00-443b-ce95-ea41adb4650a"
   },
   "outputs": [],
   "source": [
    " #Print first 5 rows of the Dataset\n",
    "heart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hl2NznXc9UF",
    "outputId": "b1e36518-6dd8-403a-807a-57d5d155c94f"
   },
   "outputs": [],
   "source": [
    "# Numbers of Rows and Columns in the Dataset\n",
    "\n",
    "heart_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5MgJaZqdGG8",
    "outputId": "3d7a6efb-dfec-409e-ff5e-b78e8538a76d"
   },
   "outputs": [],
   "source": [
    "# Getting some info about the Data\n",
    "\n",
    "heart_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "3fVz7NZfJXSE",
    "outputId": "3b8ba167-27ff-4093-87ad-bd1cf0893476"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=heart_data['ChestPain'], order=heart_data['ChestPain'].value_counts().index)\n",
    "plt.title(\"Chest Pain Type Distribution\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "cYop6hbwJmCr",
    "outputId": "8219da94-847d-4bae-cb5b-6b81d1d1ed4b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "pd.crosstab(heart_data['Sex'], heart_data['AHD']).plot(kind='bar', stacked=True, colormap=\"coolwarm\", figsize=(8, 5))\n",
    "plt.title(\"Stacked Bar Plot of Sex vs. Heart Disease\")\n",
    "plt.xlabel(\"Sex (0 = Female, 1 = Male)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"AHD\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "MLTBXnhpJ6te",
    "outputId": "82be65eb-2d88-4dd2-d3ff-9cb4610f8691"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=heart_data['Thal'], order=heart_data['Thal'].value_counts().index)\n",
    "plt.title(\"Thalassemia Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "wbf5ebVLKH6T",
    "outputId": "c329cf61-1d1c-4734-cf71-76894badd4c3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(x=heart_data['AHD'], y=heart_data['Age'])\n",
    "plt.title(\"Age Distribution by Heart Disease\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "6eL7dg-7GOxM",
    "outputId": "1d41e90e-420f-42ed-9c94-4a227dd763bc"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "# Iterate through columns and encode object (string) type columns\n",
    "for col in heart_data.select_dtypes(include=['object']).columns:\n",
    "    heart_data[col] = encoder.fit_transform(heart_data[col])\n",
    "\n",
    "# Now calculate the correlation and plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heart_data.corr(), annot=True)  # Generate the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "-ezj4oW4dNVP",
    "outputId": "5db5932c-f620-4cb5-8b4a-fc626ce01a67"
   },
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "heart_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSmrzcVsdNzH",
    "outputId": "59016aa9-b9d2-4918-8c8b-59cc646fddc7"
   },
   "outputs": [],
   "source": [
    "# Fill missing values with the mean\n",
    "heart_data.fillna(heart_data.mean(), inplace=True)\n",
    "\n",
    "# Check for remaining null values\n",
    "print(heart_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "Pid_P8yHdRZW",
    "outputId": "eb08be71-8904-4f84-c6ca-f527d73e1bb6"
   },
   "outputs": [],
   "source": [
    "#Statistical measures about the data\n",
    "\n",
    "heart_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "PvzhMkQjdUPm",
    "outputId": "676dcd24-caae-4b95-b387-90082cec9d01"
   },
   "outputs": [],
   "source": [
    "# Checking the distribution of Target Variable\n",
    "\n",
    "heart_data['AHD'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvqPaTS4dWgb",
    "outputId": "5e0431ef-ce98-4373-b6f6-7051df51e4a7"
   },
   "outputs": [],
   "source": [
    "X = heart_data.drop(columns='AHD',axis=1)\n",
    "Y = heart_data['AHD']\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyTB2zs7dZJF",
    "outputId": "41589bc9-eddf-47a3-c3bd-7c0742c21eb7"
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRMJbo2xSw2u",
    "outputId": "cea4b417-4b3c-42f0-8e16-5d08f4a4d4ce"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred) * 100\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "    print(f\"{name} Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"{name} Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Classification Report for {name} on Test Data:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8Rm4p6h5DyQ",
    "outputId": "2d1bfdee-74a4-401d-ae0f-035cf16c2bea"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the Whale Optimization Algorithm\n",
    "def whale_optimization(obj_function, bounds, whales=10, iterations=10, a=2):\n",
    "    dim = len(bounds)\n",
    "    positions = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds], (whales, dim))\n",
    "\n",
    "    # Convert categorical values to discrete options\n",
    "    for i in range(whales):\n",
    "        positions[i, 1] = round(positions[i, 1])  # Solver index\n",
    "        positions[i, 2] = round(positions[i, 2])  # Penalty index\n",
    "\n",
    "    fitness = np.array([obj_function(ind) for ind in positions])\n",
    "    best_idx = np.argmax(fitness)\n",
    "    best_position = positions[best_idx]\n",
    "\n",
    "    for t in range(iterations):\n",
    "        a_t = a - t * (a / iterations)\n",
    "        for i in range(whales):\n",
    "            A = 2 * a_t * np.random.rand(dim) - a_t\n",
    "            C = 2 * np.random.rand(dim)\n",
    "            p = np.random.rand()\n",
    "\n",
    "            if p < 0.5:\n",
    "                D = np.abs(C * best_position - positions[i])\n",
    "                positions[i] = best_position - A * D\n",
    "            else:\n",
    "                positions[i] = (best_position + positions[i]) / 2\n",
    "\n",
    "            # Ensure values stay within bounds\n",
    "            positions[i] = np.clip(positions[i], [b[0] for b in bounds], [b[1] for b in bounds])\n",
    "\n",
    "            # Convert categorical values\n",
    "            positions[i, 1] = round(positions[i, 1])  # Solver index\n",
    "            positions[i, 2] = round(positions[i, 2])  # Penalty index\n",
    "\n",
    "        fitness = np.array([obj_function(ind) for ind in positions])\n",
    "        best_idx = np.argmax(fitness)\n",
    "        best_position = positions[best_idx]\n",
    "\n",
    "    return best_position, fitness[best_idx]\n",
    "\n",
    "# Objective function for Logistic Regression\n",
    "def objective_function(params):\n",
    "    C, solver_idx, penalty_idx = params\n",
    "    C = max(0.01, min(10, C))  # Ensure within range\n",
    "\n",
    "    solvers = ['liblinear', 'lbfgs']\n",
    "    penalties = ['l1', 'l2']\n",
    "\n",
    "    solver = solvers[int(solver_idx) % len(solvers)]\n",
    "    penalty = penalties[int(penalty_idx) % len(penalties)]\n",
    "\n",
    "    if solver == 'lbfgs' and penalty == 'l1':\n",
    "        return 0  # lbfgs does not support L1, so we return a low score\n",
    "\n",
    "    clf = LogisticRegression(C=C, solver=solver, penalty=penalty, random_state=42, max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    return accuracy_score(y_test, predictions)\n",
    "\n",
    "# Define hyperparameter bounds for WOA\n",
    "bounds = [(0.01, 10), (0, 1), (0, 1)]  # C, solver index, penalty index\n",
    "\n",
    "# Run Whale Optimization Algorithm\n",
    "best_params, best_fitness = whale_optimization(objective_function, bounds)\n",
    "\n",
    "best_C, best_solver_idx, best_penalty_idx = best_params\n",
    "best_C = max(0.01, min(10, best_C))\n",
    "solvers = ['liblinear', 'lbfgs']\n",
    "penalties = ['l1', 'l2']\n",
    "best_solver = solvers[int(best_solver_idx) % len(solvers)]\n",
    "best_penalty = penalties[int(best_penalty_idx) % len(penalties)]\n",
    "\n",
    "# Train the optimized Logistic Regression model\n",
    "best_lr = LogisticRegression(C=best_C, solver=best_solver, penalty=best_penalty, random_state=42, max_iter=1000)\n",
    "best_lr.fit(X_train, y_train)\n",
    "final_predictions = best_lr.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_predictions) * 100\n",
    "\n",
    "# Output results\n",
    "print(\"Optimized Logistic Regression with Whale Optimization Algorithm\")\n",
    "print(f\"Best Parameters: C={best_C:.4f}, Solver={best_solver}, Penalty={best_penalty}\")\n",
    "print(f\"Final Accuracy: {final_accuracy:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "Mtz2FDCimqaN",
    "outputId": "1d31dad0-fa97-4345-fea5-8714e72f2f5c"
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, final_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Confusion Matrix - Optimized Logistic Regression')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jE3XjH7VYZyJ",
    "outputId": "fe8de5ab-b99a-4641-c69c-df8fd9bd64ae"
   },
   "outputs": [],
   "source": [
    "perm_importance = permutation_importance(best_lr, X_train, y_train)\n",
    "\n",
    "sorted_idx = (-perm_importance.importances_mean).argsort()\n",
    "\n",
    "list_of_tuples  = list(zip(X_train.columns[sorted_idx],\n",
    "                           perm_importance.importances_mean[sorted_idx]))\n",
    "\n",
    "perm_importance = pd.DataFrame(list_of_tuples,\n",
    "                  columns=['feature','permutation importance'])\n",
    "\n",
    "perm_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "id": "5FOt-jZ0sN3r",
    "outputId": "033b73be-14ba-4c92-bfec-6317638b50ac"
   },
   "outputs": [],
   "source": [
    "bigfig = plt.figure(figsize=(12,6))\n",
    "\n",
    "(top,bottom) = bigfig.subfigures(2,1)\n",
    "\n",
    "\n",
    "### Bottom figure ###\n",
    "bottom.subplots_adjust(left=.1,right=.9,wspace=.4,hspace=.4)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "sns.barplot(perm_importance[perm_importance['permutation importance'] > 0.02], x='feature', y='permutation importance')\n",
    "\n",
    "plt.title('Permutation-Based Importances > 0.02', fontsize=25)\n",
    "plt.xlabel('feature', fontsize=15)\n",
    "plt.xticks(fontsize=8, rotation=45)\n",
    "plt.ylabel('permutation importance', fontsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uiWQXhBkCGK",
    "outputId": "742e2630-822d-4fc0-b612-ca81ad39e714"
   },
   "outputs": [],
   "source": [
    "input_data = (1,63,1,0,145,233,1,2,150,0,2.3,3,0,1)\n",
    "\n",
    "# Change the input data to a numpy array\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "\n",
    "# Reshape the numpy array as we are predicting for only on instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "\n",
    "prediction = best_lr.predict(input_data_reshaped)\n",
    "print(prediction)\n",
    "\n",
    "if(prediction[0] == 0):\n",
    "  print(\"The Person does not have a Heart Disease\\n\")\n",
    "else :\n",
    "  print(\"The Person has a Heart Disease\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZ5-o_wwshqd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
